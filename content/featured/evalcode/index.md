---
date: '1'
title: 'Evaluating Exhaustiveness of Code Generated by Open Source LLMs'
cover: 'eval_code.png'
github: ''
external: 'https://drive.google.com/file/d/1hk_ieOwQRezn9ytbt_Lged5GdBQ_nkoW/view'
tech:
  - Python
  - Gemini
  - GPT-4
  - Llama 2
---
This research project enhanced evaluation techniques for code generated by Large Language Models (LLMs). Drawing from synthetic code generation research, it analyzed the accuracy and limitations of LLM-generated code, contributing nuanced perspectives to discussions on their role in software development and potential improvements in coding practices.
